**üáßüá∑ Portugu√™s** | [üá∫üá∏ English](README.en.md)
***

# Aprendizado de Caminho
*Deep Reinforcement Learning em Ve√≠culos Aut√¥nomos

[![Status](https://github.com/barrosocode/car_training/actions/workflows/blank.yml/badge.svg)](https://github.com/barrosocode/car_training/actions/workflows/blank.yml) [![MIT License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/barrosocode/car_training/blob/main/LICENSE) ![Linguagem](https://img.shields.io/github/languages/top/barrosocode/car_training)

## Resumo

Este projeto consiste no desenvolvimento de um agente de dire√ß√£o aut√¥noma utilizando Aprendizagem Profunda por Refor√ßo(Deep Reinforcement Learning). O agente foi treinado para percorrer a pista do ambiente *CarRacing* do come√ßo ao fim, aprendendo as pol√≠ticas de controle √≥timas atrav√©s de tentativa e erro.

**Principais tecnologias e m√©todos utilizados:**

* **Ambiente:** Constru√≠do e gerenciado com a biblioteca **`Gymnasium (gym)`**.
* **Algoritmo:** Proximal Policy Optimization (**PPO**) da **`Stable Baselines3`**.
* **Otimiza√ß√£o de Treinamento:** Vetoriza√ß√£o de ambientes com `make_vec_env` para paralelismo e **`VecFrameStack`** para fornecer mem√≥ria visual ao agente.
* **Pr√©-processamento:** Wrappers customizados (`gym.Wrapper`) e manipula√ß√£o de arrays com **`NumPy`**.
* **Persist√™ncia do Modelo:** Salvamento de checkpoints durante o treino com **`CheckpointCallback`**.

## Introdu√ß√£o
**Trabalho da Unidade III - Disciplina: Vis√£o Computacional 2025.1**

A dire√ß√£o aut√¥noma representa um dos desafios mais significativos e transformadores no campo da intelig√™ncia artificial e da engenharia. O desenvolvimento de ve√≠culos capazes de navegar em ambientes complexos de forma segura e eficiente tem o potencial de revolucionar os sistemas de transporte, reduzir acidentes e otimizar o fluxo de tr√°fego. Abordagens tradicionais para essa tarefa frequentemente dependem de um conjunto massivo de regras pr√©-programadas e sensores caros, tornando-as fr√°geis e de dif√≠cil generaliza√ß√£o para cen√°rios inesperados.

Como uma alternativa poderosa, a **Aprendizagem Profunda por Refor√ßo (DRL)** surge como uma metodologia promissora, inspirada na forma como humanos aprendem: atrav√©s da intera√ß√£o e da experi√™ncia. Em vez de seguir instru√ß√µes expl√≠citas, um agente de DRL aprende a tomar as melhores a√ß√µes por meio de um sistema de recompensas e penalidades, otimizando sua pol√≠tica de decis√£o ao longo do tempo.

Este projeto aplica os conceitos de DRL para treinar um agente a dirigir um carro no ambiente de simula√ß√£o **`CarRacing-v2`** da biblioteca `Gymnasium`. Este ambiente √© particularmente desafiador por exigir que o agente aprenda a controlar o ve√≠culo (a√ß√µes cont√≠nuas de volante, acelera√ß√£o e freio) baseando-se unicamente em dados visuais brutos (pixels), simulando o que uma c√¢mera a bordo de um carro real capturaria.

Os objetivos espec√≠ficos deste trabalho s√£o:
1.  Configurar e customizar o ambiente `CarRacing-v2` para o treinamento de um agente de DRL.
2.  Implementar e treinar um agente utilizando o algoritmo **Proximal Policy Optimization (PPO)**, reconhecido por sua robustez e efici√™ncia em ambientes de controle cont√≠nuo.
3.  Desenvolver um modelo final capaz de completar a pista de forma aut√¥noma, demonstrando um comportamento de dire√ß√£o coerente.
4.  Documentar o processo, a arquitetura e os resultados como um guia pr√°tico para a aplica√ß√£o de `Stable Baselines3` em problemas de controle baseados em vis√£o.

## Metodologia

Esta se√ß√£o detalha a arquitetura do sistema de treinamento e a organiza√ß√£o estrutural do c√≥digo-fonte, que foram projetadas para garantir um fluxo de trabalho claro e reprodut√≠vel.

### Arquitetura do Sistema

A arquitetura do projeto √© centrada no ciclo de intera√ß√£o padr√£o de um agente de Aprendizagem por Refor√ßo, onde o Agente e o Ambiente trocam informa√ß√µes continuamente. Esse processo √© orquestrado pela biblioteca `Stable Baselines3`, que abstrai grande parte da complexidade do loop de treinamento.

O fluxo de dados e processos pode ser visualizado no diagrama abaixo:

````mermaid
graph TD;
    A["Ambiente Gymnasium (CarRacing-v2)"] -- Observa√ß√£o (Frame) --> B["Pr√©-processamento (Wrappers)"];
    B -- Estado Processado --> C["Agente PPO (Rede Neural)"];
    C -- "A√ß√µes (Volante, Acel., Freio)" --> A;
    C -- "Dados de Treino" --> D["Loop de Treinamento (Stable Baselines3)"];
    D -- "Atualiza√ß√£o de Pesos" --> C;
    style A fill:#11111,stroke:#333,stroke-width:2px;
    style C fill:#11111,stroke:#333,stroke-width:2px;
````


# Resultados
A experimenta√ß√£o foi conduzida em duas etapas sequenciais de treinamento, visando a otimiza√ß√£o iterativa da pol√≠tica do agente.
- Fase 1 - Treinamento Explorat√≥rio: A fase inicial de treinamento produziu modelos promissores, com destaque para os checkpoints salvos em 1.500.000, 1.600.000 e 2.800.000 passos. Apesar de alcan√ßarem pontua√ß√µes parciais elevadas, esses modelos apresentavam uma falha cr√≠tica: a tend√™ncia a entrar em ciclos de comportamento sub-√≥timo. Tais ciclos impediam a explora√ß√£o de pol√≠ticas mais eficazes, resultando em um plat√¥ no desempenho.
- Fase 2 - Treinamento Refinado: Para solucionar o problema de estagna√ß√£o, a segunda fase introduziu modifica√ß√µes metodol√≥gicas. Foram ajustados os hiperpar√¢metros relacionados √† explora√ß√£o e √† fun√ß√£o de recompensa, adicionando termos que penalizam a falta de progresso. Os agentes resultantes desta fase demonstraram uma capacidade superior de evitar os ciclos de erro, levando a um aprendizado mais robusto e a um desempenho final significativamente melhor.

# Instala√ß√£o
### 1. Pr√©-requisitos

Antes de instalar as depend√™ncias Python, certifique-se de ter os seguintes softwares instalados:

* **Python 3.8 ou superior**: [Download Python](https://www.python.org/downloads/)
* **Git**: [Download Git](https://git-scm.com/downloads/)

#### **Pr√©-requisitos Espec√≠ficos do Sistema Operacional:**

<details>
<summary><strong>Para Linux (baseado em Debian/Ubuntu)</strong></summary>

Voc√™ precisar√° das ferramentas de compila√ß√£o essenciais e da biblioteca SWIG, que s√£o depend√™ncias para o ambiente `CarRacing`.

```bash
sudo apt-get update
sudo apt-get install -y build-essential swig
```
</details>

<details>
<summary><strong>Para Windows</strong></summary>

A instala√ß√£o no Windows requer algumas ferramentas de compila√ß√£o C++ para a biblioteca `Box2D`.

1.  **Microsoft C++ Build Tools**:
    * Fa√ßa o download do [Visual Studio Installer](https://visualstudio.microsoft.com/visual-cpp-build-tools/).
    * Execute o instalador e, na aba "Cargas de Trabalho", selecione a op√ß√£o **"Desenvolvimento para desktop com C++"**.
    * Prossiga com a instala√ß√£o.

2.  **SWIG**:
    * Fa√ßa o download do **SWIG for Windows** (procure por `swigwin`) no [site oficial](http://swig.org/download.html).
    * Descompacte o arquivo (ex: em `C:\swigwin`).
    * Adicione a pasta descompactada ao **PATH** do seu sistema para que o `pip` possa encontr√°-la.
        * Pesquise por "Editar as vari√°veis de ambiente do sistema" no Windows.
        * Clique em "Vari√°veis de Ambiente...".
        * Na se√ß√£o "Vari√°veis do sistema", selecione a vari√°vel `Path` e clique em "Editar".
        * Clique em "Novo" e adicione o caminho para a pasta do SWIG (ex: `C:\swigwin`).
</details>
<br>

---

### 2. Instala√ß√£o do Projeto

Siga os passos abaixo no seu terminal ou PowerShell.

**a. Clone o reposit√≥rio:**
```bash
git clone [https://github.com/SEU_USUARIO/SEU_REPOSITORIO.git](https://github.com/SEU_USUARIO/SEU_REPOSITORIO.git)
cd SEU_REPOSITORIO
```

**b. Crie e ative um ambiente virtual (Recomendado):**
```bash
# Cria o ambiente virtual
python -m venv venv

# Ativa o ambiente
# No Windows (PowerShell):
venv\Scripts\Activate.ps1
# No Linux ou Windows (Git Bash):
source venv/bin/activate
```

**c. Instale as bibliotecas Python:**

O arquivo `requirements.txt` original cont√©m pacotes espec√≠ficos de GPU para Linux. **N√£o o utilize diretamente**. Siga a op√ß√£o correspondente ao seu hardware.

<details>
<summary><strong>Op√ß√£o A: Instala√ß√£o com GPU NVIDIA (Recomendado para Treino)</strong></summary>

Esta op√ß√£o utiliza a acelera√ß√£o da sua placa de v√≠deo NVIDIA para um treinamento muito mais r√°pido.

1.  **Instale o PyTorch com suporte a CUDA:**
    Visite o [site oficial do PyTorch](https://pytorch.org/get-started/locally/) para obter o comando de instala√ß√£o exato para sua vers√£o do CUDA. Para CUDA 12.1, o comando geralmente √©:
    ```bash
    pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
    ```

2.  **Instale as outras depend√™ncias:**
    ```bash
    pip install stable_baselines3[extra] gymnasium[box2d] tensorboard
    ```
    * `stable_baselines3[extra]` instala a biblioteca com suas depend√™ncias comuns.
    * `gymnasium[box2d]` garante a instala√ß√£o correta do ambiente `CarRacing`.

</details>

<details>
<summary><strong>Op√ß√£o B: Instala√ß√£o apenas com CPU</strong></summary>

Use esta op√ß√£o se voc√™ n√£o tem uma placa de v√≠deo NVIDIA ou n√£o deseja configurar o CUDA. O treinamento ser√° significativamente mais lento.

1.  **Instale a vers√£o CPU do PyTorch:**
    ```bash
    pip install torch torchvision torchaudio
    ```

2.  **Instale as outras depend√™ncias:**
    ```bash
    pip install stable_baselines3[extra] gymnasium[box2d] tensorboard
    ```
</details>
<br>

---

### 3. Execu√ß√£o

**a. Para treinar um novo modelo:**

Execute o script de treinamento. O progresso ser√° exibido no terminal, e os modelos e logs ser√£o salvos nas pastas `car_racing_ppo_models/` e `car_racing_ppo_tensorboard/`[cite: 2].
```bash
python training.py
```
* O script `training.py` est√° configurado para executar 2.400.000 passos de tempo e salvar um checkpoint a cada 100.000 passos[cite: 2].
* O modelo usa a pol√≠tica "CnnPolicy" e v√°rios hiperpar√¢metros como `learning_rate=0.0003`, `gamma=0.99` e `batch_size=64`[cite: 2].

**b. Para avaliar um modelo j√° treinado:**

Execute o script de teste. Uma janela do Pygame aparecer√° mostrando o carro sendo controlado pelo agente.
```bash
python test.py
```
* **Importante**: Antes de executar, abra o arquivo `test.py` e verifique se a vari√°vel `model_path` aponta para o modelo (`.zip`) que voc√™ deseja testar.
* O ambiente de teste √© configurado para renderizar em modo `human` para visualiza√ß√£o.

---

### Contribuidores

<table>
  <tr>
    <td align="center">
      <a href="https://github.com/Ag0ds"><img src="https://github.com/Ag0ds.png?size=100" alt="Foto de Gabriel Arthur"/><br/><sub><b>Gabriel Arthur</b></sub></a><br/><sub>Desenvolvedor</sub>
    </td>
    <td align="center">
        <a href="https://github.com/barrosocode"><img src="https://github.com/barrosocode.png?size=100" alt="Foto de SGabriel Barroso"/><br/><sub><b>Gabriel Barroso</b></sub></a><br/><sub>Desenvolvedor</sub>
    </td>
    <td align="center">
        <a href="https://github.com/heltonmaia"><img src="https://github.com/heltonmaia.png?size=100" alt="Foto de Helton Maia"/><br/><sub><b>Helton Maia</b></sub></a><br/><sub>Orientador</sub>
    </td>
  </tr>
</table>

---
